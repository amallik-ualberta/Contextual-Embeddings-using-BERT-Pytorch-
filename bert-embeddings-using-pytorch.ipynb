{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('Device:', torch.cuda.get_device_name(0))\n \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","execution_count":3,"outputs":[{"output_type":"stream","text":"No GPU available, using the CPU instead.\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',\n                                          do_lower_case=False)\n \n# need all hidden states to get last 4 layers\npretrained = BertModel.from_pretrained('bert-base-multilingual-cased',\n                                       output_hidden_states=True) \n        \n# move pretrained model to device\npretrained = pretrained.to(device)\n","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f13d29470f74d1fabd3d9127c66f052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d533dbc92ef487693afc18be6dbf2b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd691d9ca67a4158b2de3539a77eee97"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocess:\n \n    '''\n    Tokenize sentences and obtain hidden representations for annotated tokens \n    '''\n    def __init__(self, tokenizer, pretrained):\n \n        '''\n        Args:\n        tokenizer: transformers’ tokenizer object\n        pretrained: transformers’ model object\n        '''\n \n        self.tokenizer = tokenizer\n        self.pretrained = pretrained\n        \n    def __call__(self, sentence, ids):\n \n        '''\n        Args:\n        sentence (List[str]): list of tokens\n        ids (List[int and str]): List of instance ids for annotation (-1 if other)\n        '''\n \n        tokenizer = self.tokenizer\n \n        tokenized_sent = [torch.tensor([tokenizer.encode(tokenizer.cls_token,\n                          add_special_tokens=False)])] # add [CLS] token\n        annotation_mask = [-1] \n        for tok, idx in zip(sentence, ids):\n\t     # tokenizer.encode returns list of token ids\n            word_ids = torch.tensor([tokenizer.encode(tok, \n                                     add_special_tokens=False)])             \n            # ignore empty / invalid tokens\n            if word_ids.size() == torch.Size([1, 0]):\n                continue\n \n            tokenized_sent.append(word_ids)\n \n            if idx != -1: # annotated instance → mask it for later \n                annotation_mask.extend([1] * (word_ids.size(dim=-1))) \n \n            else: # other\n                annotation_mask.extend([-1] * word_ids.size(dim=-1))\n \n        tokenized_sent.append(torch.tensor([tokenizer.encode(tokenizer.sep_token, \n                              add_special_tokens=False)])) # add [SEP] token\n        annotation_mask.append(-1)\n \n        # concatenate tensors to create 1 continuing tensor \n        # (format: tensor([[a, b, c]]))\n        tokenized_sent = torch.cat(tokenized_sent, dim=-1)\n        \n        # run pretrained model to get hidden representations \n        # - tokenized_sent = list of token ids\n        # - annotation_mask = list of -1 or 1 \n        #   (tokens corresponding to the annotated word is 1 and -1 otherwise)\n        annotated_emb = self.get_embeddings(tokenized_sent, annotation_mask)\n \n        return annotated_emb\n        \n    def get_embeddings(self, tokenized_sent, annotation_mask, freeze=True):\n \n        pretrained = self.pretrained\n \n        # freeze parameters in BERT\n        if freeze:\n            for param in pretrained.parameters():\n                param.requires_grad = False\n \n        # get hidden representations\n        annotated_embs = []\n \n        tokenized_sent = tokenized_sent.to(device)\n        # run pretrained model to get hidden representations\n        # output format = (last_hidden_state, pooler_output, \n        #                  hidden_states[optional], attentions[optional])\n        with torch.no_grad():\n            embs = pretrained(tokenized_sent)\n \n        # hidden states = input_emb + hidden_state_embs \n        # --> hidden_state_embs = [1:]\n        hidden_embs = embs[2][1:] \n        for index, mask in enumerate(annotation_mask):\n            if mask == 1:\n                # need to squeeze() to match dimensions \n                # (size: [1, #token] --> [#token])\n                out = sum([hidden_embs[i].squeeze()[index] \n                           for i in [-1, -2, -3, -4]])\n                annotated_embs.append(out.cpu())\n \n        annotated_emb = torch.mean(torch.stack(annotated_embs), dim=0)\n \n        return annotated_emb\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess = Preprocess(tokenizer, pretrained)\n\n'''\nAssume we have a sentence “I go to the bank to deposit some money .”, \nand the word “bank” is annotated (we want the embedding for “bank”).\n'''\n \nsentence = 'I go to the bank to deposit some money .'\nsentence = sentence.split(' ')\nannotated_id = 4 # 5-th word “bank” is annotated\n \nids = [-1] * len(sentence)\nids[annotated_id] = 1 # mask the annotated word\n \nctx_emb = preprocess(sentence, ids)\n\nprint(ctx_emb)\n","execution_count":11,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}